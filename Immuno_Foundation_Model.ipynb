{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a169f3b-896a-4658-9aa1-2e302662d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ImmunoFoundationModel - JupyterLab single-file version\n",
    "\n",
    "Research-grade Python code for multimodal modeling of women's reproductive health,\n",
    "focused on reproductive aging / ovarian reserve and endometriosis risk.\n",
    "\n",
    "This script integrates:\n",
    "- Data schemas and preprocessing\n",
    "- Multi-modal PyTorch Dataset\n",
    "- Modality-specific encoders and ImmunoFoundationModel\n",
    "- Multitask training engine + optional masked-feature pretraining\n",
    "- Integrated gradients interpretability\n",
    "- Example configs and a synthetic test pipeline\n",
    "\n",
    "IMPORTANT:\n",
    "- No biological constants or thresholds are hard-coded.\n",
    "- Wherever clinical cut points are needed, insert them manually as `# TODO` with real values.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml\n",
    "from captum.attr import IntegratedGradients\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def load_config(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"Load YAML configuration from disk.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def configure_logging(level: int = logging.INFO) -> None:\n",
    "    \"\"\"Configure basic logging.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Data schemas\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModalitySchema:\n",
    "    \"\"\"Schema describing a data modality.\n",
    "\n",
    "    Attributes:\n",
    "        name: Name of the modality (e.g., \"methylation\", \"immune\").\n",
    "        id_column: Identifier column shared across modalities.\n",
    "        feature_columns: List of feature column names used for the modality.\n",
    "        categorical_columns: Optional list of categorical columns (for clinical data).\n",
    "        column_mapping: Mapping from raw column names to canonical internal names.\n",
    "    \"\"\"\n",
    "\n",
    "    name: str\n",
    "    id_column: str = \"patient_id\"\n",
    "    feature_columns: List[str] = field(default_factory=list)\n",
    "    categorical_columns: Optional[List[str]] = None\n",
    "    column_mapping: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskSchema:\n",
    "    \"\"\"Schema describing supervised tasks.\n",
    "\n",
    "    Attributes correspond to column names in the merged dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    reproductive_age: Optional[str] = None\n",
    "    oocyte_yield: Optional[str] = None\n",
    "    endometriosis_presence: Optional[str] = None\n",
    "    endometriosis_stage: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DatasetSchema:\n",
    "    \"\"\"Container for dataset schemas across modalities and tasks.\"\"\"\n",
    "\n",
    "    modalities: Dict[str, ModalitySchema]\n",
    "    tasks: TaskSchema\n",
    "\n",
    "\n",
    "def create_schema_from_config(config: Dict[str, Any]) -> DatasetSchema:\n",
    "    \"\"\"Create DatasetSchema from YAML-style config dict.\"\"\"\n",
    "    modalities: Dict[str, ModalitySchema] = {}\n",
    "    for name, modality_cfg in config[\"data\"][\"modalities\"].items():\n",
    "        modalities[name] = ModalitySchema(\n",
    "            name=name,\n",
    "            id_column=modality_cfg.get(\"id_column\", \"patient_id\"),\n",
    "            feature_columns=modality_cfg.get(\"feature_columns\", []),\n",
    "            categorical_columns=modality_cfg.get(\"categorical_columns\"),\n",
    "            column_mapping=modality_cfg.get(\"column_mapping\", {}),\n",
    "        )\n",
    "    tasks_cfg = config[\"data\"].get(\"tasks\", {})\n",
    "    tasks = TaskSchema(\n",
    "        reproductive_age=tasks_cfg.get(\"reproductive_age\"),\n",
    "        oocyte_yield=tasks_cfg.get(\"oocyte_yield\"),\n",
    "        endometriosis_presence=tasks_cfg.get(\"endometriosis_presence\"),\n",
    "        endometriosis_stage=tasks_cfg.get(\"endometriosis_stage\"),\n",
    "    )\n",
    "    return DatasetSchema(modalities=modalities, tasks=tasks)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Preprocessing\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    \"\"\"Handle preprocessing across multiple modalities.\n",
    "\n",
    "    The preprocessor fits scalers/encoders on the training set only and reuses\n",
    "    them for validation/test or inference. Preprocessing artifacts are saved\n",
    "    to disk so the exact transformations can be applied later.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, schema: DatasetSchema, artifacts_dir: Path):\n",
    "        self.schema = schema\n",
    "        self.artifacts_dir = Path(artifacts_dir)\n",
    "        self.artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.transformers: Dict[str, Pipeline] = {}\n",
    "        self.feature_names_: Dict[str, List[str]] = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_table(path: Path) -> pd.DataFrame:\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"Expected data file at {path}\")\n",
    "        if path.suffix.lower() in {\".parquet\", \".pq\"}:\n",
    "            return pd.read_parquet(path)\n",
    "        return pd.read_csv(path)\n",
    "\n",
    "    def load_modality(\n",
    "        self,\n",
    "        modality_name: str,\n",
    "        path: Path,\n",
    "        extra_columns: Optional[List[str]] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Load a single modality table, apply column mapping, and trim columns.\"\"\"\n",
    "        modality: ModalitySchema = self.schema.modalities[modality_name]\n",
    "        df = self._load_table(path)\n",
    "\n",
    "        # Apply column mapping if provided\n",
    "        if modality.column_mapping:\n",
    "            df = df.rename(columns=modality.column_mapping)\n",
    "\n",
    "        extras = extra_columns or []\n",
    "        retained_extras = [col for col in extras if col in df.columns]\n",
    "\n",
    "        if modality.feature_columns:\n",
    "            expected = set(modality.feature_columns + [modality.id_column])\n",
    "            missing = expected - set(df.columns)\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing columns for {modality_name}: {missing}\")\n",
    "            selected_cols = [modality.id_column] + modality.feature_columns + retained_extras\n",
    "            # Deduplicate while preserving order\n",
    "            unique_cols = list(dict.fromkeys(selected_cols))\n",
    "            df = df[unique_cols]\n",
    "        else:\n",
    "            # If no feature list provided, treat all non-id columns as features\n",
    "            df = df.dropna(axis=1, how=\"all\")\n",
    "            cols = [c for c in df.columns if c != modality.id_column]\n",
    "            modality.feature_columns = cols\n",
    "\n",
    "        return df\n",
    "\n",
    "    def merge_modalities(self, modality_frames: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "        \"\"\"Inner-join all modalities on their id column; drop rows with any NaNs.\"\"\"\n",
    "        base: Optional[pd.DataFrame] = None\n",
    "        for name, frame in modality_frames.items():\n",
    "            id_col = self.schema.modalities[name].id_column\n",
    "            if base is None:\n",
    "                base = frame\n",
    "            else:\n",
    "                base = base.merge(frame, on=id_col, how=\"inner\")\n",
    "        if base is None:\n",
    "            raise ValueError(\"No modalities provided for merge\")\n",
    "        base = base.dropna(axis=0, how=\"any\")\n",
    "        logger.info(\"Merged modalities shape: %s\", base.shape)\n",
    "        return base\n",
    "\n",
    "    def _build_transformer(self, modality_name: str) -> Pipeline:\n",
    "        modality = self.schema.modalities[modality_name]\n",
    "        categorical = modality.categorical_columns or []\n",
    "        continuous = [c for c in modality.feature_columns if c not in categorical]\n",
    "        transformers = []\n",
    "        if continuous:\n",
    "            transformers.append((\"continuous\", StandardScaler(), continuous))\n",
    "        if categorical:\n",
    "            transformers.append(\n",
    "                (\n",
    "                    \"categorical\",\n",
    "                    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                    categorical,\n",
    "                )\n",
    "            )\n",
    "        if not transformers:\n",
    "            raise ValueError(f\"No transformers configured for {modality_name}\")\n",
    "        return ColumnTransformer(transformers)\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"Fit transformers on feature columns for each modality.\"\"\"\n",
    "        for modality_name, modality in self.schema.modalities.items():\n",
    "            transformer = self._build_transformer(modality_name)\n",
    "            transformer.fit(df[modality.feature_columns])\n",
    "            self.transformers[modality_name] = transformer\n",
    "\n",
    "            feature_names: List[str] = []\n",
    "            for name, trans, cols in transformer.transformers_:\n",
    "                if name == \"categorical\" and hasattr(trans, \"get_feature_names_out\"):\n",
    "                    feature_names.extend(trans.get_feature_names_out(cols).tolist())\n",
    "                elif isinstance(cols, list):\n",
    "                    feature_names.extend(cols)\n",
    "            self.feature_names_[modality_name] = feature_names\n",
    "            logger.info(\n",
    "                \"Fitted transformer for %s with %d features\",\n",
    "                modality_name,\n",
    "                len(feature_names),\n",
    "            )\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Transform the dataframe into numeric arrays by modality.\"\"\"\n",
    "        transformed: Dict[str, np.ndarray] = {}\n",
    "        for modality_name, transformer in self.transformers.items():\n",
    "            modality = self.schema.modalities[modality_name]\n",
    "            transformed[modality_name] = transformer.transform(df[modality.feature_columns])\n",
    "        return transformed\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Convenience method: fit on df, then transform df.\"\"\"\n",
    "        self.fit(df)\n",
    "        return self.transform(df)\n",
    "\n",
    "    def split(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        test_size: float,\n",
    "        val_size: float,\n",
    "        random_state: int,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Randomly split the merged dataframe into train/val/test sets.\"\"\"\n",
    "        train_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        relative_val = val_size / max(1e-8, (1 - test_size))\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_df,\n",
    "            test_size=relative_val,\n",
    "            random_state=random_state,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    def save_artifacts(self) -> None:\n",
    "        \"\"\"Save transformers and feature names to disk.\"\"\"\n",
    "        for modality_name, transformer in self.transformers.items():\n",
    "            path = self.artifacts_dir / f\"{modality_name}_transformer.pkl\"\n",
    "            with open(path, \"wb\") as f:\n",
    "                pickle.dump(transformer, f)\n",
    "        meta = {\"feature_names\": self.feature_names_}\n",
    "        with open(self.artifacts_dir / \"preprocess_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        logger.info(\"Saved preprocessing artifacts to %s\", self.artifacts_dir)\n",
    "\n",
    "    def load_artifacts(self) -> None:\n",
    "        \"\"\"Load transformers and feature names from disk.\"\"\"\n",
    "        feature_meta_path = self.artifacts_dir / \"preprocess_meta.json\"\n",
    "        with open(feature_meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        self.feature_names_ = meta.get(\"feature_names\", {})\n",
    "        for modality_name in self.schema.modalities:\n",
    "            path = self.artifacts_dir / f\"{modality_name}_transformer.pkl\"\n",
    "            with open(path, \"rb\") as f:\n",
    "                self.transformers[modality_name] = pickle.load(f)\n",
    "        logger.info(\"Loaded preprocessing artifacts from %s\", self.artifacts_dir)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Dataset and collator\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class MultiModalDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for multi-modal tabular data.\n",
    "\n",
    "    Args:\n",
    "        inputs: Dict mapping modality names to numpy arrays of shape (n_samples, n_features).\n",
    "        targets: Dict mapping task names to numpy arrays of targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs: Dict[str, np.ndarray], targets: Dict[str, np.ndarray]):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.length = next(iter(inputs.values())).shape[0]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        batch: Dict[str, torch.Tensor] = {\n",
    "            modality: torch.as_tensor(values[idx]).float()\n",
    "            for modality, values in self.inputs.items()\n",
    "        }\n",
    "        for task, target in self.targets.items():\n",
    "            batch[task] = torch.as_tensor(target[idx])\n",
    "        return batch\n",
    "\n",
    "\n",
    "class BatchCollator:\n",
    "    \"\"\"Collate function to keep modalities grouped when batching.\"\"\"\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        collated: Dict[str, torch.Tensor] = {}\n",
    "        for key in batch[0].keys():\n",
    "            collated[key] = torch.stack([item[key] for item in batch], dim=0)\n",
    "        return collated\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Models: encoders and foundation model\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def build_mlp(input_dim: int, hidden_dims: Tuple[int, ...], dropout: float = 0.1) -> nn.Sequential:\n",
    "    \"\"\"Build a simple MLP with BatchNorm and Dropout.\"\"\"\n",
    "    layers: List[nn.Module] = []\n",
    "    prev_dim = input_dim\n",
    "    for hidden_dim in hidden_dims:\n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "            ]\n",
    "        )\n",
    "        prev_dim = hidden_dim\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class MethylationEncoder(nn.Module):\n",
    "    \"\"\"Encoder for DNA methylation features using a simple MLP.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(input_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ImmuneEncoder(nn.Module):\n",
    "    \"\"\"Encoder for immune markers and cytokines.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(input_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MitoEncoder(nn.Module):\n",
    "    \"\"\"Encoder for mitochondrial features.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(input_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class ClinicalEncoder(nn.Module):\n",
    "    \"\"\"Encoder for clinical and lifestyle covariates.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, hidden_dims: Tuple[int, ...]):\n",
    "        super().__init__()\n",
    "        self.net = build_mlp(input_dim, hidden_dims)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def encoder_dispatch(modality: str, input_dim: int, hidden_dims: Tuple[int, ...]) -> nn.Module:\n",
    "    \"\"\"Return appropriate encoder module for a given modality name.\"\"\"\n",
    "    mapping: Dict[str, nn.Module] = {\n",
    "        \"methylation\": MethylationEncoder(input_dim, hidden_dims),\n",
    "        \"immune\": ImmuneEncoder(input_dim, hidden_dims),\n",
    "        \"mitochondrial\": MitoEncoder(input_dim, hidden_dims),\n",
    "        \"clinical\": ClinicalEncoder(input_dim, hidden_dims),\n",
    "    }\n",
    "    if modality not in mapping:\n",
    "        raise ValueError(f\"Unsupported modality {modality}\")\n",
    "    return mapping[modality]\n",
    "\n",
    "\n",
    "class ImmunoFoundationModel(nn.Module):\n",
    "    \"\"\"Multimodal foundation model with modality-specific encoders and multitask heads.\n",
    "\n",
    "    Outputs:\n",
    "        - reproductive_age: continuous regression\n",
    "        - oocyte_yield: continuous regression\n",
    "        - endometriosis_presence: binary classification (logits)\n",
    "        - endometriosis_stage: optional continuous/ordinal regression (if enabled)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims: Dict[str, int],\n",
    "        encoder_hidden: Tuple[int, ...] = (128, 64),\n",
    "        backbone_hidden: Tuple[int, ...] = (128, 64),\n",
    "        dropout: float = 0.1,\n",
    "        enable_stage: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleDict()\n",
    "        latent_dims = 0\n",
    "        for modality, input_dim in input_dims.items():\n",
    "            encoder = encoder_dispatch(modality, input_dim, encoder_hidden)\n",
    "            self.encoders[modality] = encoder\n",
    "            latent_dims += encoder_hidden[-1] if encoder_hidden else input_dim\n",
    "\n",
    "        self.backbone = build_mlp(latent_dims, backbone_hidden, dropout=dropout)\n",
    "        backbone_out = backbone_hidden[-1] if backbone_hidden else latent_dims\n",
    "\n",
    "        self.heads = nn.ModuleDict(\n",
    "            {\n",
    "                \"reproductive_age\": nn.Linear(backbone_out, 1),\n",
    "                \"oocyte_yield\": nn.Linear(backbone_out, 1),\n",
    "                \"endometriosis_presence\": nn.Linear(backbone_out, 1),\n",
    "            }\n",
    "        )\n",
    "        if enable_stage:\n",
    "            self.heads[\"endometriosis_stage\"] = nn.Linear(backbone_out, 1)\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:  # type: ignore[override]\n",
    "        latent: List[torch.Tensor] = []\n",
    "        for modality, encoder in self.encoders.items():\n",
    "            latent.append(encoder(inputs[modality]))\n",
    "        fused = torch.cat(latent, dim=1)\n",
    "        shared = self.backbone(fused)\n",
    "        outputs: Dict[str, torch.Tensor] = {}\n",
    "        for task, head in self.heads.items():\n",
    "            outputs[task] = head(shared)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class MaskedFeatureModel(nn.Module):\n",
    "    \"\"\"Self-supervised module for masked feature reconstruction.\"\"\"\n",
    "\n",
    "    def __init__(self, foundation: ImmunoFoundationModel, input_dims: Dict[str, int]):\n",
    "        super().__init__()\n",
    "        self.foundation = foundation\n",
    "        self.reconstructors = nn.ModuleDict()\n",
    "        for mod, dim in input_dims.items():\n",
    "            encoder = foundation.encoders[mod]\n",
    "            latent_dim: Optional[int] = None\n",
    "            # Infer latent dimension from the last Linear layer in encoder.net\n",
    "            for layer in reversed(encoder.net):  # type: ignore[attr-defined]\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    latent_dim = layer.out_features\n",
    "                    break\n",
    "            if latent_dim is None:\n",
    "                raise ValueError(f\"Unable to infer latent dimension for modality {mod}\")\n",
    "            self.reconstructors[mod] = nn.Linear(latent_dim, dim)\n",
    "\n",
    "    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:  # type: ignore[override]\n",
    "        latents = {mod: encoder(inputs[mod]) for mod, encoder in self.foundation.encoders.items()}\n",
    "        reconstructions = {mod: self.reconstructors[mod](latent) for mod, latent in latents.items()}\n",
    "        return reconstructions\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Metrics\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Return MAE, RMSE, and R^2 for regression.\"\"\"\n",
    "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
    "    rmse = metrics.mean_squared_error(y_true, y_pred, squared=False)\n",
    "    r2 = metrics.r2_score(y_true, y_pred)\n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "\n",
    "def classification_metrics(y_true: np.ndarray, y_pred_logits: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Return AUROC, AUPRC, accuracy, sensitivity, and specificity for binary classification.\"\"\"\n",
    "    y_prob = 1 / (1 + np.exp(-y_pred_logits))\n",
    "    if len(np.unique(y_true)) > 1:\n",
    "        auroc = metrics.roc_auc_score(y_true, y_prob)\n",
    "    else:\n",
    "        auroc = float(\"nan\")\n",
    "    precision, recall, _ = metrics.precision_recall_curve(y_true, y_prob)\n",
    "    auprc = metrics.auc(recall, precision)\n",
    "    preds = (y_prob >= 0.5).astype(int)\n",
    "    acc = metrics.accuracy_score(y_true, preds)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, preds, labels=[0, 1]).ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float(\"nan\")\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float(\"nan\")\n",
    "    return {\n",
    "        \"auroc\": auroc,\n",
    "        \"auprc\": auprc,\n",
    "        \"accuracy\": acc,\n",
    "        \"sensitivity\": sensitivity,\n",
    "        \"specificity\": specificity,\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Training engine\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    epochs: int = 5\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    batch_size: int = 32\n",
    "    device: str = \"cpu\"\n",
    "    grad_clip: float = 1.0\n",
    "    patience: int = 3\n",
    "    modality_dropout: float = 0.0  # placeholder if you want to expand\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Trainer handling multitask supervised learning.\"\"\"\n",
    "\n",
    "    def __init__(self, model: ImmunoFoundationModel, config: TrainingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = torch.device(config.device)\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "        self.lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode=\"min\",\n",
    "            patience=2,\n",
    "            factor=0.5,\n",
    "        )\n",
    "        self.loss_fns: Dict[str, nn.Module] = {\n",
    "            \"reproductive_age\": nn.MSELoss(),\n",
    "            \"oocyte_yield\": nn.MSELoss(),\n",
    "            \"endometriosis_presence\": nn.BCEWithLogitsLoss(),\n",
    "            \"endometriosis_stage\": nn.MSELoss(),\n",
    "        }\n",
    "\n",
    "    def _compute_loss(self, outputs: Dict[str, torch.Tensor], batch: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        losses: List[torch.Tensor] = []\n",
    "        for task, pred in outputs.items():\n",
    "            if task not in batch:\n",
    "                continue\n",
    "            target = batch[task].float().to(self.device)\n",
    "            pred = pred.view_as(target)\n",
    "            losses.append(self.loss_fns[task](pred, target))\n",
    "        if not losses:\n",
    "            raise ValueError(\"No losses computed; check task configuration\")\n",
    "        return torch.stack(losses).mean()\n",
    "\n",
    "    def train_epoch(self, loader: DataLoader) -> float:\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(self.device) for k, v in batch.items() if k not in self.loss_fns}\n",
    "            targets = {k: v for k, v in batch.items() if k in self.loss_fns}\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self._compute_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        return running_loss / len(loader)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, loader: DataLoader) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        all_outputs: Dict[str, List[np.ndarray]] = {task: [] for task in self.loss_fns}\n",
    "        all_targets: Dict[str, List[np.ndarray]] = {task: [] for task in self.loss_fns}\n",
    "        total_loss = 0.0\n",
    "        batches = 0\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(self.device) for k, v in batch.items() if k not in self.loss_fns}\n",
    "            targets = {k: v for k, v in batch.items() if k in self.loss_fns}\n",
    "            outputs = self.model(inputs)\n",
    "            loss = self._compute_loss(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            batches += 1\n",
    "            for task, pred in outputs.items():\n",
    "                if task not in targets:\n",
    "                    continue\n",
    "                all_outputs[task].append(pred.cpu().numpy())\n",
    "                all_targets[task].append(targets[task].cpu().numpy())\n",
    "        metrics_report: Dict[str, float] = {\"loss\": total_loss / max(1, batches)}\n",
    "        for task, preds in all_outputs.items():\n",
    "            if not preds or not all_targets[task]:\n",
    "                continue\n",
    "            y_true = np.concatenate(all_targets[task])\n",
    "            y_pred = np.concatenate(preds)\n",
    "            if task in {\"reproductive_age\", \"oocyte_yield\", \"endometriosis_stage\"}:\n",
    "                task_metrics = regression_metrics(y_true, y_pred)\n",
    "            else:\n",
    "                task_metrics = classification_metrics(y_true, y_pred)\n",
    "            for k, v in task_metrics.items():\n",
    "                metrics_report[f\"{task}_{k}\"] = v\n",
    "        return metrics_report\n",
    "\n",
    "    def fit(self, train_loader: DataLoader, val_loader: DataLoader, output_dir: Path) -> Dict[str, float]:\n",
    "        best_val = float(\"inf\")\n",
    "        patience_counter = 0\n",
    "        best_state: Optional[Dict[str, torch.Tensor]] = None\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        val_metrics: Dict[str, float] = {}\n",
    "        for epoch in range(self.config.epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_metrics = self.evaluate(val_loader)\n",
    "            self.lr_scheduler.step(val_metrics[\"loss\"])\n",
    "            logger.info(\n",
    "                \"Epoch %d train_loss=%.4f val_loss=%.4f\",\n",
    "                epoch + 1,\n",
    "                train_loss,\n",
    "                val_metrics[\"loss\"],\n",
    "            )\n",
    "            if val_metrics[\"loss\"] < best_val:\n",
    "                best_val = val_metrics[\"loss\"]\n",
    "                best_state = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= self.config.patience:\n",
    "                    logger.info(\"Early stopping at epoch %d\", epoch + 1)\n",
    "                    break\n",
    "        if best_state:\n",
    "            torch.save(best_state, output_dir / \"best_model.pt\")\n",
    "        return val_metrics\n",
    "\n",
    "\n",
    "def pretrain_masked_reconstruction(\n",
    "    model: ImmunoFoundationModel,\n",
    "    loader: DataLoader,\n",
    "    input_dims: Dict[str, int],\n",
    "    device: str,\n",
    "    epochs: int,\n",
    "    mask_prob: float = 0.15,\n",
    ") -> None:\n",
    "    \"\"\"Simple masked feature reconstruction pretraining.\"\"\"\n",
    "    mask_module = MaskedFeatureModel(model, input_dims).to(device)\n",
    "    optimizer = optim.Adam(mask_module.parameters(), lr=1e-3)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k not in model.heads}\n",
    "            masked_inputs: Dict[str, torch.Tensor] = {}\n",
    "            targets: Dict[str, torch.Tensor] = {}\n",
    "            for modality, tensor in inputs.items():\n",
    "                mask = torch.rand_like(tensor) < mask_prob\n",
    "                masked = tensor.clone()\n",
    "                masked[mask] = 0.0\n",
    "                masked_inputs[modality] = masked\n",
    "                targets[modality] = tensor\n",
    "            optimizer.zero_grad()\n",
    "            reconstructions = mask_module(masked_inputs)\n",
    "            losses = [loss_fn(reconstructions[m], targets[m]) for m in reconstructions]\n",
    "            loss = torch.stack(losses).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        logger.info(\"Pretrain epoch %d loss=%.4f\", epoch + 1, epoch_loss / len(loader))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Interpretability with Integrated Gradients\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def compute_integrated_gradients(\n",
    "    model: torch.nn.Module,\n",
    "    inputs: Dict[str, torch.Tensor],\n",
    "    target_head: str,\n",
    "    baseline: float = 0.0,\n",
    "    n_steps: int = 32,\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Compute integrated gradients for each modality.\n",
    "\n",
    "    Args:\n",
    "        model: Trained ImmunoFoundationModel.\n",
    "        inputs: Dictionary of modality -> tensor with batch dimension.\n",
    "        target_head: Task head name to explain.\n",
    "        baseline: Baseline value for IG.\n",
    "        n_steps: Number of interpolation steps.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    inputs_clone = {k: v.clone().requires_grad_(True) for k, v in inputs.items()}\n",
    "\n",
    "    def forward_func(*tensors: torch.Tensor) -> torch.Tensor:\n",
    "        packed = {name: tensor for name, tensor in zip(inputs_clone.keys(), tensors)}\n",
    "        outputs = model(packed)\n",
    "        if target_head not in outputs:\n",
    "            raise ValueError(f\"Task head {target_head} not found in model outputs\")\n",
    "        return outputs[target_head]\n",
    "\n",
    "    ig = IntegratedGradients(forward_func)\n",
    "    tensor_inputs = tuple(inputs_clone.values())\n",
    "    baselines = tuple(torch.full_like(t, baseline) for t in tensor_inputs)\n",
    "    ig_values = ig.attribute(tensor_inputs, baselines=baselines, n_steps=n_steps)\n",
    "    attributions: Dict[str, torch.Tensor] = {}\n",
    "    for name, attr in zip(inputs_clone.keys(), ig_values):\n",
    "        attributions[name] = attr.detach()\n",
    "    return attributions\n",
    "\n",
    "\n",
    "def summarize_feature_importance(attributions: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Aggregate absolute attributions per feature for global ranking.\"\"\"\n",
    "    summary = {modality: attr.abs().mean(dim=0) for modality, attr in attributions.items()}\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# High-level helpers for training/inference (CLI-style but callable in notebook)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def prepare_targets(config: Dict[str, Any], df: pd.DataFrame) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Extract target arrays from dataframe based on config['data']['tasks'].\"\"\"\n",
    "    targets: Dict[str, np.ndarray] = {}\n",
    "    tasks = config[\"data\"].get(\"tasks\", {})\n",
    "    for task_name, column in tasks.items():\n",
    "        if column is None:\n",
    "            continue\n",
    "        if column not in df.columns:\n",
    "            raise ValueError(f\"Expected target column {column} for task {task_name}\")\n",
    "        targets[task_name] = df[column].to_numpy()\n",
    "    return targets\n",
    "\n",
    "\n",
    "def build_dataloaders(\n",
    "    config: Dict[str, Any],\n",
    "    preprocessor: DataPreprocessor,\n",
    "    train_df: pd.DataFrame,\n",
    "    val_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"Build train/val/test dataloaders from preprocessed dataframes.\"\"\"\n",
    "    train_inputs = preprocessor.transform(train_df)\n",
    "    val_inputs = preprocessor.transform(val_df)\n",
    "    test_inputs = preprocessor.transform(test_df)\n",
    "\n",
    "    targets_train = prepare_targets(config, train_df)\n",
    "    targets_val = prepare_targets(config, val_df)\n",
    "    targets_test = prepare_targets(config, test_df)\n",
    "\n",
    "    train_ds = MultiModalDataset(train_inputs, targets_train)\n",
    "    val_ds = MultiModalDataset(val_inputs, targets_val)\n",
    "    test_ds = MultiModalDataset(test_inputs, targets_test)\n",
    "\n",
    "    batch_size = config[\"training\"].get(\"batch_size\", 32)\n",
    "    drop_last = config[\"training\"].get(\"drop_last\", False)\n",
    "    collator = BatchCollator()\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collator, drop_last=drop_last),\n",
    "        DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collator, drop_last=False),\n",
    "        DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collator, drop_last=False),\n",
    "    )\n",
    "\n",
    "\n",
    "def run_training(config_path: Path) -> None:\n",
    "    \"\"\"End-to-end training using a YAML config file (can be called from a notebook).\"\"\"\n",
    "    config = load_config(config_path)\n",
    "    configure_logging()\n",
    "    set_seed(config.get(\"seed\", 42))\n",
    "    artifacts_dir = Path(config.get(\"artifacts_dir\", \"artifacts\"))\n",
    "    artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    schema = create_schema_from_config(config)\n",
    "    preprocessor = DataPreprocessor(schema, artifacts_dir / \"preprocess\")\n",
    "\n",
    "    modality_frames: Dict[str, pd.DataFrame] = {}\n",
    "    task_columns = [col for col in config[\"data\"].get(\"tasks\", {}).values() if col]\n",
    "    for name, modality_cfg in config[\"data\"][\"modalities\"].items():\n",
    "        modality_frames[name] = preprocessor.load_modality(\n",
    "            name,\n",
    "            Path(modality_cfg[\"path\"]),\n",
    "            extra_columns=task_columns,\n",
    "        )\n",
    "    merged = preprocessor.merge_modalities(modality_frames)\n",
    "\n",
    "    train_df, val_df, test_df = preprocessor.split(\n",
    "        merged,\n",
    "        test_size=config[\"data\"].get(\"test_size\", 0.2),\n",
    "        val_size=config[\"data\"].get(\"val_size\", 0.1),\n",
    "        random_state=config.get(\"seed\", 42),\n",
    "    )\n",
    "\n",
    "    preprocessor.fit(train_df)\n",
    "    preprocessor.save_artifacts()\n",
    "\n",
    "    train_loader, val_loader, test_loader = build_dataloaders(config, preprocessor, train_df, val_df, test_df)\n",
    "\n",
    "    input_dims = {\n",
    "        name: train_loader.dataset.inputs[name].shape[1]  # type: ignore[arg-type]\n",
    "        for name in train_loader.dataset.inputs  # type: ignore[attr-defined]\n",
    "    }\n",
    "    model = ImmunoFoundationModel(\n",
    "        input_dims=input_dims,\n",
    "        encoder_hidden=tuple(config[\"model\"].get(\"encoder_hidden\", [128, 64])),\n",
    "        backbone_hidden=tuple(config[\"model\"].get(\"backbone_hidden\", [128, 64])),\n",
    "        dropout=config[\"model\"].get(\"dropout\", 0.1),\n",
    "        enable_stage=config[\"data\"].get(\"tasks\", {}).get(\"endometriosis_stage\") is not None,\n",
    "    )\n",
    "\n",
    "    training_cfg = TrainingConfig(\n",
    "        epochs=config[\"training\"].get(\"epochs\", 5),\n",
    "        lr=config[\"training\"].get(\"lr\", 1e-3),\n",
    "        weight_decay=config[\"training\"].get(\"weight_decay\", 1e-4),\n",
    "        batch_size=config[\"training\"].get(\"batch_size\", 32),\n",
    "        device=config[\"training\"].get(\"device\", \"cpu\"),\n",
    "        grad_clip=config[\"training\"].get(\"grad_clip\", 1.0),\n",
    "        patience=config[\"training\"].get(\"patience\", 3),\n",
    "        modality_dropout=config[\"training\"].get(\"modality_dropout\", 0.0),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model, training_cfg)\n",
    "\n",
    "    if config[\"training\"].get(\"pretrain_masked\", False):\n",
    "        pretrain_masked_reconstruction(\n",
    "            model,\n",
    "            train_loader,\n",
    "            input_dims=input_dims,\n",
    "            device=training_cfg.device,\n",
    "            epochs=config[\"training\"].get(\"pretrain_epochs\", 3),\n",
    "            mask_prob=config[\"training\"].get(\"mask_prob\", 0.15),\n",
    "        )\n",
    "\n",
    "    val_metrics = trainer.fit(train_loader, val_loader, artifacts_dir / \"checkpoints\")\n",
    "    test_metrics = trainer.evaluate(test_loader)\n",
    "\n",
    "    with open(artifacts_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"val\": val_metrics, \"test\": test_metrics}, f, indent=2)\n",
    "    logger.info(\"Saved metrics to %s\", artifacts_dir / \"metrics.json\")\n",
    "\n",
    "\n",
    "def run_inference(config_path: Path, input_path: Path, model_checkpoint: Path) -> Dict[str, List[float]]:\n",
    "    \"\"\"Run inference on new data using existing preprocessing artifacts and checkpoint.\"\"\"\n",
    "    config = load_config(config_path)\n",
    "    configure_logging()\n",
    "    schema = create_schema_from_config(config)\n",
    "    preprocessor = DataPreprocessor(schema, Path(config.get(\"artifacts_dir\", \"artifacts\")) / \"preprocess\")\n",
    "    preprocessor.load_artifacts()\n",
    "\n",
    "    modality_frames: Dict[str, pd.DataFrame] = {}\n",
    "    for name, modality_cfg in config[\"data\"][\"modalities\"].items():\n",
    "        # Assume files share the same filename as in training config, but under input_path\n",
    "        original_path = Path(modality_cfg[\"path\"])\n",
    "        filename = original_path.name\n",
    "        modality_frames[name] = preprocessor.load_modality(name, input_path / filename)\n",
    "\n",
    "    merged = preprocessor.merge_modalities(modality_frames)\n",
    "    inputs = preprocessor.transform(merged)\n",
    "\n",
    "    input_dims = {name: arr.shape[1] for name, arr in inputs.items()}\n",
    "    model = ImmunoFoundationModel(\n",
    "        input_dims=input_dims,\n",
    "        encoder_hidden=tuple(config[\"model\"].get(\"encoder_hidden\", [128, 64])),\n",
    "        backbone_hidden=tuple(config[\"model\"].get(\"backbone_hidden\", [128, 64])),\n",
    "        dropout=config[\"model\"].get(\"dropout\", 0.1),\n",
    "        enable_stage=config[\"data\"].get(\"tasks\", {}).get(\"endometriosis_stage\") is not None,\n",
    "    )\n",
    "    state = torch.load(model_checkpoint, map_location=\"cpu\")\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    tensors = {k: torch.tensor(v).float() for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensors)\n",
    "    results = {k: np.squeeze(v.numpy()).tolist() for k, v in outputs.items()}\n",
    "    print(json.dumps(results, indent=2))\n",
    "    return results\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Example configs as Python dicts (you can save them to YAML if you want)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "example_endometriosis_config: Dict[str, Any] = {\n",
    "    \"seed\": 7,\n",
    "    \"artifacts_dir\": \"artifacts/endometriosis\",\n",
    "    \"data\": {\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.1,\n",
    "        \"modalities\": {\n",
    "            \"clinical\": {\n",
    "                \"path\": \"data/patients.csv\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [\"age_years\", \"bmi\", \"amh_ng_ml\", \"afc\"],\n",
    "                \"categorical_columns\": [\"ethnicity\", \"smoking_status\", \"parity\"],\n",
    "                \"column_mapping\": {},\n",
    "            },\n",
    "            \"methylation\": {\n",
    "                \"path\": \"data/methylation.parquet\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [],  # TODO: fill with real CpG/region features\n",
    "            },\n",
    "            \"immune\": {\n",
    "                \"path\": \"data/immune.csv\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [],  # TODO: fill with real immune markers\n",
    "            },\n",
    "            \"mitochondrial\": {\n",
    "                \"path\": \"data/mitochondrial.csv\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [],  # TODO: fill with mitochondrial scores\n",
    "            },\n",
    "        },\n",
    "        \"tasks\": {\n",
    "            \"reproductive_age\": None,\n",
    "            \"oocyte_yield\": None,\n",
    "            \"endometriosis_presence\": \"clinical_endometriosis\",\n",
    "            \"endometriosis_stage\": \"endometriosis_stage\",  # TODO: ensure clinical staging schema\n",
    "        },\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"encoder_hidden\": [64, 32],\n",
    "        \"backbone_hidden\": [64, 32],\n",
    "        \"dropout\": 0.15,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 8,\n",
    "        \"lr\": 0.0005,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"batch_size\": 16,\n",
    "        \"device\": \"cpu\",\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"patience\": 3,\n",
    "        \"pretrain_masked\": False,\n",
    "    },\n",
    "}\n",
    "\n",
    "example_reproductive_aging_config: Dict[str, Any] = {\n",
    "    \"seed\": 42,\n",
    "    \"artifacts_dir\": \"artifacts/reproductive_aging\",\n",
    "    \"data\": {\n",
    "        \"test_size\": 0.2,\n",
    "        \"val_size\": 0.1,\n",
    "        \"modalities\": {\n",
    "            \"clinical\": {\n",
    "                \"path\": \"data/patients.csv\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [\n",
    "                    \"age_years\",\n",
    "                    \"bmi\",\n",
    "                    \"amh_ng_ml\",\n",
    "                    \"afc\",\n",
    "                    \"oocyte_yield\",\n",
    "                ],\n",
    "                \"categorical_columns\": [\n",
    "                    \"ethnicity\",\n",
    "                    \"smoking_status\",\n",
    "                    \"parity\",\n",
    "                    \"cycle_phase\",\n",
    "                ],\n",
    "                \"column_mapping\": {},\n",
    "            },\n",
    "            \"methylation\": {\n",
    "                \"path\": \"data/methylation.parquet\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [],  # TODO: CpG or region aggregate columns\n",
    "            },\n",
    "            \"immune\": {\n",
    "                \"path\": \"data/immune.csv\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [],  # TODO: immune markers and cytokines\n",
    "            },\n",
    "            \"mitochondrial\": {\n",
    "                \"path\": \"data/mitochondrial.csv\",\n",
    "                \"id_column\": \"patient_id\",\n",
    "                \"feature_columns\": [],  # TODO: mitochondrial scores\n",
    "            },\n",
    "        },\n",
    "        \"tasks\": {\n",
    "            \"reproductive_age\": \"reproductive_age_label\",  # TODO: replace with real column\n",
    "            \"oocyte_yield\": \"oocyte_yield\",\n",
    "            \"endometriosis_presence\": \"clinical_endometriosis\",\n",
    "            \"endometriosis_stage\": None,\n",
    "        },\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"encoder_hidden\": [128, 64],\n",
    "        \"backbone_hidden\": [128, 64],\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 10,\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0.0001,\n",
    "        \"batch_size\": 32,\n",
    "        \"device\": \"cpu\",\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"patience\": 3,\n",
    "        \"pretrain_masked\": True,\n",
    "        \"pretrain_epochs\": 2,\n",
    "        \"mask_prob\": 0.15,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def save_config_to_yaml(config: Dict[str, Any], path: Path) -> None:\n",
    "    \"\"\"Utility to save one of the example configs to a YAML file.\"\"\"\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(config, f)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Synthetic pipeline demo (non-biological, just to validate code paths)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def run_synthetic_pipeline_demo(base_dir: Path = Path(\"synthetic_demo\")) -> None:\n",
    "    \"\"\"Create tiny synthetic tables, run preprocessing + one epoch of training.\n",
    "\n",
    "    This uses non-biological placeholder data to validate shapes and code paths.\n",
    "    \"\"\"\n",
    "    base_dir.mkdir(parents=True, exist_ok=True)\n",
    "    data_dir = base_dir / \"data\"\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Synthetic tables\n",
    "    patient_ids = [f\"p{i}\" for i in range(12)]\n",
    "    clinical = pd.DataFrame(\n",
    "        {\n",
    "            \"patient_id\": patient_ids,\n",
    "            \"age_years\": np.linspace(30, 40, len(patient_ids)),\n",
    "            \"bmi\": np.linspace(20, 28, len(patient_ids)),\n",
    "            \"amh_ng_ml\": np.linspace(1, 3, len(patient_ids)),\n",
    "            \"afc\": np.linspace(10, 20, len(patient_ids)),\n",
    "            \"oocyte_yield\": np.linspace(5, 15, len(patient_ids)),\n",
    "            \"clinical_endometriosis\": np.random.randint(0, 2, len(patient_ids)),\n",
    "            \"repro_age_target\": np.linspace(32, 45, len(patient_ids)),\n",
    "        }\n",
    "    )\n",
    "    methylation = pd.DataFrame(\n",
    "        {\n",
    "            \"patient_id\": patient_ids,\n",
    "            \"meth_1\": np.random.rand(len(patient_ids)),\n",
    "            \"meth_2\": np.random.rand(len(patient_ids)),\n",
    "        }\n",
    "    )\n",
    "    immune = pd.DataFrame(\n",
    "        {\n",
    "            \"patient_id\": patient_ids,\n",
    "            \"immune_1\": np.random.rand(len(patient_ids)),\n",
    "            \"immune_2\": np.random.rand(len(patient_ids)),\n",
    "        }\n",
    "    )\n",
    "    mitochondrial = pd.DataFrame(\n",
    "        {\n",
    "            \"patient_id\": patient_ids,\n",
    "            \"mito_1\": np.random.rand(len(patient_ids)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    paths = {\n",
    "        \"clinical\": data_dir / \"patients.csv\",\n",
    "        \"methylation\": data_dir / \"methylation.csv\",\n",
    "        \"immune\": data_dir / \"immune.csv\",\n",
    "        \"mitochondrial\": data_dir / \"mitochondrial.csv\",\n",
    "    }\n",
    "    clinical.to_csv(paths[\"clinical\"], index=False)\n",
    "    methylation.to_csv(paths[\"methylation\"], index=False)\n",
    "    immune.to_csv(paths[\"immune\"], index=False)\n",
    "    mitochondrial.to_csv(paths[\"mitochondrial\"], index=False)\n",
    "\n",
    "    config = {\n",
    "        \"seed\": 123,\n",
    "        \"artifacts_dir\": str(base_dir / \"artifacts\"),\n",
    "        \"data\": {\n",
    "            \"test_size\": 0.2,\n",
    "            \"val_size\": 0.1,\n",
    "            \"modalities\": {\n",
    "                \"clinical\": {\n",
    "                    \"path\": str(paths[\"clinical\"]),\n",
    "                    \"id_column\": \"patient_id\",\n",
    "                    \"feature_columns\": [\n",
    "                        \"age_years\",\n",
    "                        \"bmi\",\n",
    "                        \"amh_ng_ml\",\n",
    "                        \"afc\",\n",
    "                        \"oocyte_yield\",\n",
    "                    ],\n",
    "                    \"categorical_columns\": [],\n",
    "                    \"column_mapping\": {},\n",
    "                },\n",
    "                \"methylation\": {\n",
    "                    \"path\": str(paths[\"methylation\"]),\n",
    "                    \"id_column\": \"patient_id\",\n",
    "                    \"feature_columns\": [\"meth_1\", \"meth_2\"],\n",
    "                },\n",
    "                \"immune\": {\n",
    "                    \"path\": str(paths[\"immune\"]),\n",
    "                    \"id_column\": \"patient_id\",\n",
    "                    \"feature_columns\": [\"immune_1\", \"immune_2\"],\n",
    "                },\n",
    "                \"mitochondrial\": {\n",
    "                    \"path\": str(paths[\"mitochondrial\"]),\n",
    "                    \"id_column\": \"patient_id\",\n",
    "                    \"feature_columns\": [\"mito_1\"],\n",
    "                },\n",
    "            },\n",
    "            \"tasks\": {\n",
    "                \"reproductive_age\": \"repro_age_target\",\n",
    "                \"oocyte_yield\": \"oocyte_yield\",\n",
    "                \"endometriosis_presence\": \"clinical_endometriosis\",\n",
    "                \"endometriosis_stage\": None,\n",
    "            },\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"encoder_hidden\": [32, 16],\n",
    "            \"backbone_hidden\": [32, 16],\n",
    "            \"dropout\": 0.1,\n",
    "        },\n",
    "        \"training\": {\n",
    "            \"epochs\": 1,\n",
    "            \"lr\": 1e-3,\n",
    "            \"batch_size\": 4,\n",
    "            \"device\": \"cpu\",\n",
    "            \"patience\": 2,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    schema = create_schema_from_config(config)\n",
    "    preprocessor = DataPreprocessor(schema, Path(config[\"artifacts_dir\"]) / \"preprocess\")\n",
    "\n",
    "    task_columns = [col for col in config[\"data\"][\"tasks\"].values() if col]\n",
    "    modality_frames = {\n",
    "        name: preprocessor.load_modality(name, Path(mod_cfg[\"path\"]), extra_columns=task_columns)\n",
    "        for name, mod_cfg in config[\"data\"][\"modalities\"].items()\n",
    "    }\n",
    "    merged = preprocessor.merge_modalities(modality_frames)\n",
    "    train_df, val_df, test_df = preprocessor.split(merged, test_size=0.2, val_size=0.1, random_state=123)\n",
    "    preprocessor.fit(train_df)\n",
    "\n",
    "    train_inputs = preprocessor.transform(train_df)\n",
    "    targets = {\n",
    "        task: train_df[col].to_numpy()\n",
    "        for task, col in config[\"data\"][\"tasks\"].items()\n",
    "        if col is not None\n",
    "    }\n",
    "\n",
    "    dataset = MultiModalDataset(train_inputs, targets)\n",
    "    loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=BatchCollator(), drop_last=True)\n",
    "\n",
    "    input_dims = {name: arr.shape[1] for name, arr in train_inputs.items()}\n",
    "    model = ImmunoFoundationModel(\n",
    "        input_dims=input_dims,\n",
    "        encoder_hidden=(16, 8),\n",
    "        backbone_hidden=(16, 8),\n",
    "        dropout=0.1,\n",
    "        enable_stage=False,\n",
    "    )\n",
    "    trainer = Trainer(model, TrainingConfig(epochs=1, batch_size=2))\n",
    "\n",
    "    initial_outputs = model(\n",
    "        {\n",
    "            k: torch.tensor(v[:2]).float()\n",
    "            for k, v in train_inputs.items()\n",
    "        }\n",
    "    )\n",
    "    assert set(initial_outputs.keys()) >= {\"reproductive_age\", \"oocyte_yield\", \"endometriosis_presence\"}\n",
    "    for tensor in initial_outputs.values():\n",
    "        assert tensor.shape[0] == 2\n",
    "\n",
    "    train_loss = trainer.train_epoch(loader)\n",
    "    print(f\"Synthetic demo training loss: {train_loss:.4f}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Optional CLI entry point (works in a .py script; harmless in notebook)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(description=\"Train or run inference with the ImmunoFoundationModel\")\n",
    "    parser.add_argument(\"--config\", type=Path, required=False, help=\"Path to YAML configuration file\")\n",
    "    parser.add_argument(\"--mode\", choices=[\"train\", \"inference\"], default=\"train\")\n",
    "    parser.add_argument(\"--input_dir\", type=Path, help=\"Directory with modality files for inference\")\n",
    "    parser.add_argument(\"--checkpoint\", type=Path, help=\"Model checkpoint for inference\")\n",
    "    parser.add_argument(\"--synthetic_demo\", action=\"store_true\", help=\"Run synthetic pipeline demo instead\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.synthetic_demo:\n",
    "        run_synthetic_pipeline_demo()\n",
    "        return\n",
    "\n",
    "    if args.config is None:\n",
    "        raise ValueError(\"Please provide --config or use --synthetic_demo\")\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        run_training(args.config)\n",
    "    else:\n",
    "        if args.input_dir is None or args.checkpoint is None:\n",
    "            raise ValueError(\"Inference mode requires --input_dir and --checkpoint\")\n",
    "        run_inference(args.config, args.input_dir, args.checkpoint)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In a notebook this block will not be executed unless you explicitly run\n",
    "    # %run this_script.py from the terminal. Safe to leave as-is.\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f24ab-f496-422f-b59b-051d2681479e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
